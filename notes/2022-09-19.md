---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.14.1
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Getting Started with Exploratory Data Analysis

Now we get to start actual data science!


Our goal this week is to explore the actual data, the values, in a dataset to get a basic idea of what is in the data.  

Summarizing and Visualizing Data are **very** important

- People cannot interpret high dimensional or large samples quickly
- Important in EDA to help you make decisions about the rest of your analysis
- Important in how you report your results
- Summaries are similar calculations to performance metrics we will see later
- visualizations are often essential in debugging models


**THEREFORE**
- You have  [a lot of chances](https://rhodyprog4ds.github.io/BrownFall22/syllabus/achievements.html#assignments-and-skills) to earn summarize and visualize
- we will be picky when we assess if you earned them or not
```{code-cell} ipython3
import pandas as pd
```

## Staying Organized

- See the new {ref}`resource:file-structure` section.
- Be sure to accept assignments and close the feedback PR if you will not work on them

## Loading Data and Examining Structure

```{code-cell} ipython3
coffee_data_url = 'https://raw.githubusercontent.com/jldbc/coffee-quality-database/master/data/robusta_data_cleaned.csv'
coffee_df = pd.read_csv(coffee_data_url,index_col=0)
```

So far, we've loaded data in a few different ways and then we've examined
DataFrames as a data structure, looking at what different attributes they have
and what some of the methods are, and how to get data into them.

```{code-cell} ipython3
coffee_df.head(2)
```

From here we can see a few sample values of most of the column and we can be sure that the data loaded correctly.


```{admonition} Try it Yourself
What other tools have we learned to examine a DataFrame and when might you use them?
```


We can also get more structural information with the [`info`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html) method.

More information on this can also be found in the [dtypes](https://pandas.pydata.org/docs/user_guide/basics.html#basics-dtypes) attribute. Including that the type is the [most general](https://pandas.pydata.org/docs/user_guide/basics.html#basics-dtypes:~:text=If%20a%20pandas%20object%20contains%20data%20with%20multiple%20dtypes%20in%20a%20single%20column%2C%20the%20dtype%20of%20the%20column%20will%20be%20chosen%20to%20accommodate%20all%20of%20the%20data%20types%20(object%20is%20the%20most%20general).) if there are multiple types in the column.

````{margin}
```{hint}
There is also a related [`select_dtypes`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.select_dtypes.html#pandas.DataFrame.select_dtypes) method.
```
````

```{code-cell} ipython3
coffee_df.info()
```

## Summary Statistics

Now, we can actually start to analyze the data itself.

The [`describe`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) method provides us with a set of summary statistics that broadly
describe the data overall.  

```{code-cell} ipython3
coffee_df.describe()
```

From this, we can draw several conclusions.  FOr example straightforward ones like:
- the smallest number of bags rated is 1 and at least 25% of the coffees rates only had 1 bag
- the first ratings included were 2012 and last in 2017  (min & max)
- the mean Mouthfeel was 7.5
- Category One defects are not very common ( the 75th% is 0)

Or more nuanced ones that compare across variables like
- the raters scored coffee higher on Uniformity.Cup and Clean.Cup than other scores (mean score; only on the ones that seem to have a scale of up to 8/10)
- the coffee varied more in Mouthfeel and Balance that most other scores (the std; only on the ones that seem to have a scale of of up to 8/10)
- there are 3 ratings with no altitude (count of other variables is 28; alt is 25

And these all give us a sense of the values and the distribution or spread fo the data in each column.


We can use the descriptive statistics on individual columns as well.

```{code-cell} ipython3
coffee_df['Balance'].describe()
```

To dig in on what the quantiles really mean, we can compute one manually.

First, we sort the data, then for the 25%, we select the point in index 6 because
becaues there are 28 values.

```{code-cell} ipython3
balance_sorted = coffee_df['Balance'].sort_values().values
```

What value of `x` to pick out 25%

```{code-cell} ipython3
x = 6
balance_sorted[x]
```


````{margin}
```{admonition} further reading
On the [documentation page for describe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) the "<i class="fas fa-info-circle"></i>
 See Also" shows the links to the documentation of most of the individual functions.  This is a good way to learn about other things, or find something when you are not quite sure what it would be named.  Go to a function thats similar to what you want and then look at the related functions.
```
````


We can also extract each of the statistics that the `describe` method calculates individually, by name.  The quantiles
are tricky, we cannot just `.25%()` to get the 25% percentile, we have to use the
`quantile` method and pass it a value between 0 and 1.


```{code-cell} ipython3
coffee_df['Flavor'].quantile(.8)
```

Calculate the mean of `Aftertaste`

```{code-cell} ipython3
coffee_df['Aftertaste'].mean()
```

## Describing Nonnumerical Variables

There are different columns in the describe than the the whole dataset:
```{code-cell} ipython3
coffee_df.columns
```

```{code-cell} ipython3
coffee_df.describe().columns
```

We can get the prevalence of each one with `value_counts`
```{code-cell} ipython3
coffee_df['Color'].value_counts()
```


```{admonition} Try it Yourself
Note `value_counts` does not count the `NaN` values, but `count` counts all of the
not missing values and the shape of the DataFrame is the total number of rows.
How can you get the number of missing Colors?
```

Describe only operates on the numerical columns, but we might want to know about the others.  We can get the number of each value with `value_counts`

```{code-cell} ipython3
coffee_df['Country.of.Origin'].value_counts()
```


We can get the name of the most common country out of this Series using `idmax`

```{code-cell} ipython3
coffee_df['Country.of.Origin'].value_counts().idxmax()
```


Or see only how many different values with the related:
```{code-cell} ipython3
coffee_df['Country.of.Origin'].nunique()
```

We can also use the mode function, which works on both numerical or nonnumerical

```{code-cell} ipython3
coffee_df['Country.of.Origin'].mode()
```

## Basic Plotting

Pandas give us basic plotting capability built right into DataFrames.

```{code-cell} ipython3
coffee_df['Country.of.Origin'].value_counts().plot()
```
It defaults to a line graph, which is not very informative in this case, so we can use the `kind` parameter to change it to a bar graph.

```{code-cell} ipython3
coffee_df['Country.of.Origin'].value_counts().plot(kind='bar')
```

## Matching Questions to Summary Statistics

When we brainstorm more questions that we can answer with summary statistics or that we might want to ask of this data, the ones that come to mind are often actually dependent on two variables. For example are two scores correlated? or what country has the highest rated coffe on `Flavor`?   

We'll come back to more detailed questions like this on Friday, but to start we can look at how numerical variables vary by categorical (nonnumerical) variables by grouping the data.


Above we saw which country had the most ratings (remember one row is one rating), but what if we wanted to see the mean number of bags per country?

```{code-cell} ipython3
coffee_df.groupby('Country.of.Origin')['Number.of.Bags'].mean()
```


```{important}
This data is only about coffee that was [rated by a particular agency](https://github.com/jldbc/coffee-quality-database/) it is not economic data, so we cannot, for example conclude which country *produces* the amount of data.  If we had economic dataset, a `Number.of.Bags` columns's mean would tell us exactly that, but the context of the dataset defines what a row means and therefore how we can interpret the **every single statistic** we calculate.
```




## Questions after class


### Can arrays be used in Jupyter Notebook?

Jupyter runs a fully powered python interpreter, so all python can work inside it.


#### why did coffee_df['Country.of.Origin'].max() say vietnam?

That applies a the `max` function to the `'Country.of.Origin'` column, without counting how many times the values occur.



#### Why, in the groupby function the first column is in between parenthesis nd the second one between brackets?

The inside parenthesis is the parameter of the groupby function, we could instead do it this way:

```{code-cell} ipython3
coffee_grouped = coffee_df.groupby('Country.of.Origin')
```
Then we can use `coffee_grouped` for different things

```{code-cell} ipython3
coffee_grouped.describe()
```
or as we did before
```{code-cell} ipython3
coffee_grouped['Number.of.Bags'].mean()
```

The second one is to index  ([see last week's notes for more on indexing](2022-09-16)) the DataFrame and pick out one column.  It makes the DataFrame have fewer columns before applying mean to the whole object.  

We could make a smaller DataFrame first, then group, then mean

```{code-cell} ipython3
coffee_country_bags_df= coffee_df[['Number.of.Bags','Country.of.Origin']]
coffee_country_bags_df.head()
```

There are two sets of square brackets because putting a comma it would try to index along rows with one and columns with the other, to select multiple in one dimension you need to pass a list.
```{code-cell} ipython3
type(['Number.of.Bags','Country.of.Origin'])
```

If you do not put square `[]` or curly `{}` brackets around items, Python implicitly treats them as if there are parenthesis`()`
```{code-cell} ipython3
type(('Number.of.Bags','Country.of.Origin'))
```

and tuples are treated separately .



```{code-cell} ipython3
coffee_country_bags_df.groupby('Country.of.Origin').mean()
```


#### A quick recap of how .info() counts non-null and dtype for each column would be great. Thanks!

above

#### Can you plot a graph that uses two columns in a dataset?
Yes, we will see that on Wednesday.

#### Are there any other data types as important as dataframes in pandas?

DataFrames are the main type provided by pandas, there are also Series which is conceptually a single column, groupby objects which is basically a list of (DataFrame,label) tuples, and some indexing, but all of these exist to support creating and operating on DataFrames.

#### Will we be utilizing any other packages /libraries mentioned than the ones for visualizing data that we talked about today?

We will use pandas for basic plots and seaborn for more advanced plots.  There are other libraries for visualization like plotly for interactive plots, but those are more advanced than we need or for specialized use cases. However, the way these type of libraries are designed, is that the special use case ones are for when the basic ones do not do what you need and they often have common patterns.

#### When displaying two categories like we did at the end could you measure a different thing for each category? Like count for one and mean for the other?

Yes! We will likely not do this in class, but it is a small extension from what we have covered and uses the [pandas aggregrate](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.aggregate.html) method.

```{hint}
This is expected for visualize level 3
```


#### How in depth will we go with graphing data and looking at it systemically to find patterns?

At some level, that is what we sill do for the rest of the semester.  We will not cover everything there is to know about graphing, there are whole courses on data visualization. Most of the pattern-finding we will do is machine learning.  


### Assignment

#### For assignment 2, how do I find appropriate datasets online?

I collated the ones that I recommend on the [datasets](datasets) page.  Then find something that is of interest to you.  

#### How do you import a .py file from GitHub? Is there a way to do that without downloading it locally?

It does need to be local in order to import it as a module.  

Ideally, you will work with all of your assignment repos locally either following the instructions in [the notes from last class](ghoffline)
