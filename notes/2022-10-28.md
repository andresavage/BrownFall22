---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.10.3
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Sparse and Polynomial Regression

```{code-cell} ipython3
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
sns.set_theme(font_scale=2,palette='colorblind')
```

## Exmaining Residuals

```{code-cell} ipython3
# Load the diabetes dataset
diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)
X_train,X_test, y_train,y_test = train_test_split(diabetes_X, diabetes_y ,
                         test_size=20,random_state=0)
regr_db = linear_model.LinearRegression()
regr_db.fit(X_train, y_train)
regr_db.score(X_test,y_test)
```

```{code-cell} ipython3
regr_db.__dict__
```

```{code-cell} ipython3
y_pred = regr_db.predict(X_test)
```

```{code-cell} ipython3
plt.scatter(y_test,y_pred)

plt.scatter(y_test, y_pred, color='black')
plt.plot(y_test, y_test, color='blue', linewidth=3)

# draw vertical lines frome each data point to its predict value
[plt.plot([x,x],[yp,yt], color='red', linewidth=3)
         for x, yp, yt in zip(y_test, y_pred,y_test)];

plt.xlabel('y_test')
plt.ylabel('y_pred')
```

````{important}
plotting the resiudals this way is okay for seeing how it did, but is not intepretted the same way, I've changed it for the correct interpretation.  This says that there might not be enough information for high values to predict them, *or* maybe a better model could do it. 
````

+++

We can plot the residuals, more directly too: 

```{code-cell} ipython3
test_df = pd.DataFrame(X_test,columns = ['x'+str(i) for i in range(len(X_test[0]))])
test_df['err'] = y_pred-y_test
x_df = test_df.melt(var_name = 'feature',id_vars=['err'])
x_df.head()
```

```{code-cell} ipython3
sns.relplot(data= x_df,x='value',col='feature', col_wrap=4,y='err')
```

Here, we see that the residuals for most features are pretty uniform, but a few (eg the first one) are correlated to the feature value.

+++

## Polynomial regression

Polynomial regression is still a linear problem.  Linear regression solves for
the $\beta_i$ for a $d$ dimensional problem.

$$ y = \beta_0 + \beta_1 x_1 + \ldots + \beta_d x_d = \sum_i^d \beta_i x_i$$

Quadratic regression solves for

$$ y = \beta_0 + \sum_i^d \beta_i x_i$ + \sum_j^d \sum_i^d \beta_{d+i} x_i x_j + \sum_i^d x_i^2$ $$

This is still a linear problem, we can create a new $X$ matrix that has the
polynomial values of each feature  and solve for more $\beta$ values.

We use a transformer object, which works similarly to the estimators, but does
not use targets.
First, we instantiate.

```{code-cell} ipython3
poly = PolynomialFeatures(include_bias=False,)
```

Then we can transform the data

```{code-cell} ipython3
X2_train = poly.fit_transform(X_train)
```

```{code-cell} ipython3
X2_train.shape
```

```{code-cell} ipython3
X_train.shape
```

```{code-cell} ipython3
regr_db2 = linear_model.LinearRegression()
regr_db2.fit(X2_train,y_train)
regr_db2.score(X2_test,y_test)
```

We see the performance is a little better, but let's look at the residuals again. 

```{code-cell} ipython3
X2_test = poly.transform(X_test)
y_pred2 = regr_db2.predict(X2_test)
```

```{code-cell} ipython3
regr_db2.coef_
```

```{code-cell} ipython3
test_df2 = pd.DataFrame(X2_test,columns = ['x'+str(i) for i in range(len(X2_test[0]))])
test_df2['err'] = y_pred2-y_test
x_df2 = test_df2.melt(var_name = 'feature',id_vars=['err'])
sns.relplot(data= x_df2,x='value',col='feature', col_wrap=4,y='err')
```

From this we can see that most of these features do not vary much at all. 

```{code-cell} ipython3
X2_test.shape
```

## Sparse Regression

```{code-cell} ipython3
lasso = linear_model.Lasso()
```

```{code-cell} ipython3
lasso.fit(X_train,y_train)
```

```{code-cell} ipython3
y_pred_lasso = lasso.predict(X_test)
```

```{code-cell} ipython3
lasso.coef_
```

```{code-cell} ipython3
plt.scatter(y_test,y_pred_lasso-y_test)
```

```{code-cell} ipython3
lasso.score(X_test,y_test)
```

```{code-cell} ipython3
regr_db.score(X_test,y_test)
```

```{code-cell} ipython3
lassoa2 = linear_model.Lasso(.3)
```

```{code-cell} ipython3
lassoa2.fit(X_train,y_train)
```

```{code-cell} ipython3
lassoa2.coef_
```

```{code-cell} ipython3
y_pred_lasso2 = lassoa2.predict(X_test)
plt.scatter(y_test,y_pred_lasso2-y_test)
```

```{code-cell} ipython3
lassoa2.score(X_test,y_test)
```

```{code-cell} ipython3
lassoa2.score(X_train,y_train)
```

## Combining them

```{code-cell} ipython3

```

```{code-cell} ipython3
scores = []
alpha_vals = [1,.2,.1,.05,.001,.0005]
for alpha in alpha_vals:
    lasso2 = linear_model.Lasso(alpha)
    lasso2.fit(X2_train,y_train)
    scores.append(lasso2.score(X2_test,y_test))
    
plt.plot(alpha_vals,scores,)
```

```{code-cell} ipython3
lasso2 = linear_model.Lasso(.001)
lasso2.fit(X2_train,y_train)
lasso2.score(X2_test,y_test)
```

```{code-cell} ipython3
lasso2.coef_
```

## Tips example

```{code-cell} ipython3
tips = sns.load_dataset("tips").dropna()
tips_X = tips['total_bill'].values
tips_X = tips_X[:,np.newaxis] # add an axis
tips_y = tips['tip']

tips_X_train,tips_X_test, tips_y_train, tips_y_test = train_test_split(
                     tips_X,
                     tips_y,
                     train_size=.8,
                     random_state=0)
```

In this example, we have one feature

```{code-cell} ipython3
regr_tips = linear_model.LinearRegression()
regr_tips.fit(tips_X_train,tips_y_train)
regr_tips.score(tips_X_test,tips_y_test)
```

```{code-cell} ipython3
regr_tips.coef_, regr_tips.intercept_
```

We can interpret this as that the expected tips is $1.02 + 9.7%.  

````{margin}
These people were not good tippers
````

```{code-cell} ipython3
tip_poly = PolynomialFeatures(include_bias=False,)
tips_X2_train = tip_poly.fit_transform(tips_X_train)
tips_X2_test = tip_poly.transform(tips_X_test)
```

```{code-cell} ipython3
regr2_tips = linear_model.LinearRegression()
regr2_tips.fit(tips_X2_train,tips_y_train)
regr2_tips.score(tips_X2_test,tips_y_test)
```

In this case, it doesn't do any better, so the relationship is really linear. 

```{code-cell} ipython3
plt.scatter(tips_X_test,tips_y_test)
```

## Toy example for visualization

we'll sample some random values for x and then add a coefficient to the value and the square

```{code-cell} ipython3
N = 20
slope = 3
sq = 4
sample_x = lambda num_smaples: np.random.normal(5,scale=10,size = num_smaples)
compute_y = lambda cx: slope*cx + sq*cx*cx
x_train = sample_x(N)[:,np.newaxis]
y_train = compute_y(x_train)

x_test = sample_x(N)[:,np.newaxis]
y_test = compute_y(x_test)
```

```{code-cell} ipython3
plt.scatter(x_train,y_train)
```

Now, we'll transform, since this is one value

```{code-cell} ipython3
pf = PolynomialFeatures(include_bias=False,)
x2_train = pf.fit_transform(x_train,)
x2_test = pf.transform(x_test,)

plt.plot(x_train*x_train, x2_train[:,1])
```

```{code-cell} ipython3
regr = linear_model.LinearRegression()
regr.fit(x2_train,y_train)

regr.score(x2_test,y_test)
```

```{code-cell} ipython3
regr.coef_
```

```{code-cell} ipython3

y_pred = regr.predict(x2_test)
plt.scatter(x_test,y_pred)
```

```{code-cell} ipython3

```

```{code-cell} ipython3

```
