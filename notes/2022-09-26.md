---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.14.1
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Intro to Data Cleaning



This week, we'll be cleaning data.

Cleaning data is **labor intensive** and requires making *subjective* choices.  
We'll focus on, and assess you on, manipulating data correctly, making reasonable
choices, and documenting the choices you make carefully.

We'll focus on the programming tools that get used in cleaning data in class
this week:
- reshaping data
- handling missing or incorrect values
- renaming columns

```{code-cell} ipython3
import pandas as pd
import seaborn as sns

# sns.set_theme(pallete='colorblind')
```

## Tidy Data

Read in the three csv files described below and store them in a list of dataFrames

```{code-cell} ipython3
url_base = 'https://raw.githubusercontent.com/rhodyprog4ds/rhodyds/main/data/'

datasets = ['study_a.csv','study_b.csv','study_c.csv']
```

```{code-cell} ipython3
df_list = [pd.read_csv(url_base + current) for current in datasets]
```

```{code-cell} ipython3
type(df_list)
```

```{code-cell} ipython3
type(df_list[0])
```

```{code-cell} ipython3
df_list[0]
```

```{code-cell} ipython3
df_list[1]
```

```{code-cell} ipython3

df_list[2]
```

These three all show the same data, but let's say we have two goals:
- find the average  effect per person across treatments
- find the average effect per treatment across people

This works differently for these three versions.


```{code-cell} ipython3
df_list[0].mean()
```
we get the average per treatment, but to get the average per person, we have to go across rows, which we can do here, but doesn't work as well with plotting
```{code-cell} ipython3
df_list[0].mean(axis=1)
```
and this is not well labeled.


Let's try the next one.
```{code-cell} ipython3
df_list[1].mean()
```
Now we get the average per person, but what about per treatment? again we have to go across rows instead.

```{code-cell} ipython3
df_list[1].mean(axis=1)
```



For the third one, however, we can use groupby
```{code-cell} ipython3
df_list[2].groupby('person').mean()
```


```{code-cell} ipython3
df_list[2].groupby('treatment').mean()
```

The original [Tidy Data](https://www.jstatsoft.org/article/view/v059i10) paper is worth reading to build a deeper understanding of these ideas.  


## Tidying Data


Let's reshape the first one to match the tidy one. First, we
will save it to a DataFrame, this makes things easier to read
and enables us to use the built in help in jupyter, because it can't check types too many levels into a data structure.
```{code-cell} ipython3
treat_df = df_list[0]
```

Let's look at it again, so we can see
```{code-cell} ipython3
treat_df.head()
```

```{code-cell} ipython3
df_list[0].columns
```

```{code-cell} ipython3
df_a = df_list[0]
```

```{code-cell} ipython3
df_a.melt(id_vars=['name'],value_vars=['treatmenta','treatmentb'],
          value_name='result',var_name='treatment')
```

When we melt a dataset:
- the `id_vars` stay as columns
- the data from the `value_vars` columns become the values in the `value` column
- the column names from the `value_vars` become the values in the `variable` column
- we can rename the value and the variable columns.

Let's do it for our coffee data:

```{code-cell} ipython3
arabica_data_url = 'https://raw.githubusercontent.com/jldbc/coffee-quality-database/master/data/arabica_data_cleaned.csv'
```

```{code-cell} ipython3
coffee_df = pd.read_csv(arabica_data_url)
scores_of_interest = ['Balance','Aroma','Body','Aftertaste']
```

```{code-cell} ipython3
coffee_tall = coffee_df.melt(id_vars=['Country.of.Origin','Color'],
                             value_vars=scores_of_interest
               ,var_name = 'Score')
coffee_tall.head()
```

Notice that the actual column names inside the `scores_of_interest` variable become the values in the variable column.

This one we can plot in more ways:
```{code-cell} ipython3
sns.displot(data=coffee_tall, x='value',hue='Country.of.Origin',
            col='Score',kind='kde',col_wrap=2)
```


## Filtering Data

```{code-cell} ipython3
high_prod = coffee_df[coffee_df['Number.of.Bags']>250]
high_prod.shape
```

```{code-cell} ipython3
coffee_df.shape
```

We see that filters and reduces.  We can use any boolean expression in the square brackets.

```{code-cell} ipython3
top_balance = coffee_df[coffee_df['Balance']>coffee_df['Balance'].quantile(.75)]
top_balance.shape
```
We can confirm that we got only the top 25% of balance scores:
```{code-cell} ipython3
top_balance.describe()
```


We can also use the `isin` method to filter by comparing to an iterable type

```{code-cell} ipython3
total_per_country = coffee_df.groupby('Country.of.Origin')['Number.of.Bags'].sum()
top_countries = total_per_country.sort_values(ascending=False)[:10].index
top_coffee_df = coffee_df[coffee_df['Country.of.Origin'].isin(top_countries)]
```

## Questions after class




### In the data we had about treatment a and treatment b.  Say there was another column that provided information about the age of the people.  Could we create another value variable to or would we put it in the value_vars list along with treatment a and treatmentb?
We can have multiple variables as the `id_vars` so that the dataset would have 4 columns instead of 3.


### Can we clean data within any row of the data as long as there is a column name for it?
Yes

### How to clean larger datasets

Everything we will learn will work in any context that you can load the dataset into RAM on the computer you are working on; or process it in batches.  


### with very large data sets, how can you tell id there are missing values or incorrect types present?

We use ways of checking the values, like the `info` method to check the whole column.



### Are there any prebuilt methods to identify and remove extreme outliers?

There may be, that is a good thing to look in the documentation for.  However, typically the definition of an outlier is best made within context, so the filtering strategy that we just used would be the way to remove them, after doign some EDA.


### Is there a specific metric to which a dataset must meet to consider it 'cleaned'?
It's "clean" when it will work for the analysis you want to do. This means clean enough for one analysis may not be enough for another goal.  Domain expertise is alwasy important.

### Are there any packages we may utilize that help us clean data more effectively?


### Things we will do later this week:

- adding more data to a DataFrame in jupyter notebook?
- replace values
- deal with NaN values in a dataset?

### Next week

- What is the difference between different types of merging data frames (inner, outer, left, right, etc)?
