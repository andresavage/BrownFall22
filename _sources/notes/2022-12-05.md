---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.14.1
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

# Neural Networks


We started thinking about machine learning wiht the idea that the basic idea is
that we assume that our target variable ($y_i$) is related to the features $\mathbf{x}_i$
by some function (for sample $i$):

$$ y_i =f(\mathbf{x}_i)$$

But we don't know that function exactly, so we assume a type (a decision
  tree, a boundary for SVM, a probability distribution) that has some parameters
  $\theta$ and then use a machine
  learning algorithm $\mathcal{A}$ to estimate the parameters for $f$.  In the
  decision tree the parameters are the thresholds to compare to, in the GaussianNB the parameters are the mean and variance, in SVM it's the support vectors that define the margin.  

$$\theta = \mathcal{A}(X,y) $$

That we can use to test on our test data:

$$ \hat{y}_i = f(x_i;\theta) $$

A neural net allows us to not assume a specific form for $f$ first, it does
universal function approximation.  For one hidden layer and a binary classification problem:


$$f(x) = W_2g(W_1^T x +b_1) + b_2 $$

where the function $g$ is called the activation function. so we approximate some
unknown, complicated function $f4 by taking a weighted sum of all of the inputs,
and passing those through another, known function.


```{code-cell} ipython3
from sklearn.neural_network import MLPClassifier
from sklearn import svm
import pandas as pd
import sklearn

from sklearn import datasets
import matplotlib.pyplot as plt
from sklearn import model_selection
```


We're going to use the digits dataset again.

```{code-cell} ipython3
digits = datasets.load_digits()
digits_X = digits.data
digits_y = digits.target
X_train, X_test, y_train, y_test = model_selection.train_test_split(digits_X,digits_y)
```


```{code-cell} ipython3
digits.images[0]
```

Sklearn provides an estimator for the Multi-Llayer Perceptron (MLP). We can see one with one layer to
start.

```{code-cell} ipython3
mlp = MLPClassifier(
  hidden_layer_sizes=(16),
  max_iter=100,
  alpha=1e-4,
  solver="lbfgs",
  verbose=10,
  random_state=1,
  learning_rate_init=0.1,
)
```

```{code-cell} ipython3
mlp.fit(X_train,y_train).score(X_test,y_test)
```

We can compare it  to SVM:

```{code-cell} ipython3
svm_clf = svm.SVC(gamma=0.001)
svm_clf.fit(X_train, y_train)
svm_clf.score(X_test,y_test)
```

We saw that the SVM performed a bit better, but this is a simple problem.
We can also compare these based on much they store, the number of parameters
is realted to the complexity.

```{code-cell} ipython3
import numpy as np
```

```{code-cell} ipython3
np.prod(list(svm_clf.support_vectors_.shape))
```

```{code-cell} ipython3
np.sum([np.prod(list(c.shape)) for c in mlp.coefs_])
```

```{code-cell} ipython3
mlp.coefs_
```

```{code-cell} ipython3
mlp64 = MLPClassifier(
  hidden_layer_sizes=(64),
  max_iter=100,
  alpha=1e-4,
  solver="lbfgs",
  verbose=10,
  random_state=1,
  learning_rate_init=0.1,
)
```

```{code-cell} ipython3
mlp64.fit(X_train,y_train).score(X_test,y_test)
```

```{code-cell} ipython3

```

## Questions After Class

### Roughly, how does the model know to use certain functions as the fitting becomes more complex (e.g. sin(x), ln(x), e^x)?

It does not learn an analytical form; it just approximates it.

### when doing the .score on the mlp does the limit vary or does it have a set limit on its own?



### What is tensorflow used for that scikit cant do?

Tensorflow can do more types of networks and has more options for training.  Most importantly, it has code optmizations so that you can use more complex hardware directly.

### when you say weight, what does that mean?

Weights are coefficients, or the weight of that feature.


### what is an artificial neuron?

An artificial neuron is one "unit" of calculation.  A neuron takes a weighted sum of all of its inputs (including a bias term) and passes it through an "activation function" that squashes the values of output into [0,1].

### what real life problems require tensorflow?

All modern ML applications are tensorflow, pytorch or similar.

### What do the hidden layers of the neural network represent?

We do not specify exactly what they represent up front; we can use model explanation techniques and visualization tools to examine them after the fact and try to interpret them if needed.



### What is the best way to optimize a neural net? would it be jut adding more layers?ï»¿

You could specify some of the parameters and use GridSearch as well. There are types of layers as well. We will see that later.



### Are the weights given to the hidden layers initially random?

Typically yes, they can be initialized randomly and then they are learned.

<!--
### on tensorflow playground, if we increase the weight is that increasing the amount we are feeding within the hidden layer?

### Do the neurons' layers have to be specified in the models we are going to use, or they are already specified for each model?

### Are hidden layers just a number of masks that help the function determine what the overall classification should be?
-->
