{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9769b3f",
   "metadata": {},
   "source": [
    "# Performance Metrics continued\n",
    "\n",
    "## Logistics\n",
    "\n",
    "- A6 posted ASAP!  (I promise it will be straightforward in terms of the actual code; you do need to carefully interpret though)\n",
    "- a5 grading is behind, but if it's not done by early tomorrow (eg 10am) then I'll extend the portfolio deadline accordingly\n",
    "- [Mid Semester Feedback](https://forms.gle/BdgHf8zHcXoUcw9E8)\n",
    "\n",
    "## Completing the COMPAS Audit\n",
    "\n",
    "Let's start where we left off, plus some additional imports\n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "I used a [tag](https://jupyterbook.org/en/stable/reference/cheatsheet.html#tags) on this cell in the notes so that the output with warning is not shown here.\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39baebf2",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'aif360'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics \u001b[38;5;28;01mas\u001b[39;00m skmetrics\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maif360\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics \u001b[38;5;28;01mas\u001b[39;00m fairmetrics\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maif360\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BinaryLabelDataset\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'aif360'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics as skmetrics\n",
    "from aif360 import metrics as fairmetrics\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "import seaborn as sns\n",
    "\n",
    "compas_clean_url = 'https://raw.githubusercontent.com/ml4sts/outreach-compas/main/data/compas_c.csv'\n",
    "compas_df = pd.read_csv(compas_clean_url,index_col = 'id')\n",
    "\n",
    "compas_df = pd.get_dummies(compas_df,columns=['score_text'],)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58188ba",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "We'll get a warning which is **okay** but if you run again it will go away.\n",
    "```\n",
    "\n",
    "to review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa275a2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_df\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_df' is not defined"
     ]
    }
   ],
   "source": [
    "compas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8cd066",
   "metadata": {},
   "source": [
    "Notice today we imported the sklearn.metrics module with an alias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4bb2ea3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m skmetrics\u001b[38;5;241m.\u001b[39maccuracy_score(\u001b[43mcompas_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo_year_recid\u001b[39m\u001b[38;5;124m'\u001b[39m],compas_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_text_High\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_df' is not defined"
     ]
    }
   ],
   "source": [
    "skmetrics.accuracy_score(compas_df['two_year_recid'],compas_df['score_text_High'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cca124",
   "metadata": {},
   "source": [
    "More common is to use medium or high to check accuracy (or not low) we can calulate tihs by either summing two or inverting. We'll do it as not low for now, to review using apply.\n",
    "\n",
    "```{admonition} Try it Yourself\n",
    "A good exercise to review data manipulation is to try creating the `score_text_MedHigh` column by adding the other two together (because medium or high if they're booleans is the same as medium + high if they're ints)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3d4004e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m int_not \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m a:\u001b[38;5;28mint\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m(a))\n\u001b[0;32m----> 2\u001b[0m compas_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_text_MedHigh\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcompas_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_text_Low\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(int_not)\n\u001b[1;32m      3\u001b[0m skmetrics\u001b[38;5;241m.\u001b[39maccuracy_score(compas_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo_year_recid\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      4\u001b[0m              compas_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_text_MedHigh\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_df' is not defined"
     ]
    }
   ],
   "source": [
    "int_not = lambda a:int(not(a))\n",
    "compas_df['score_text_MedHigh'] = compas_df['score_text_Low'].apply(int_not)\n",
    "skmetrics.accuracy_score(compas_df['two_year_recid'],\n",
    "             compas_df['score_text_MedHigh'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d859dd6",
   "metadata": {},
   "source": [
    "We can see this gives us a slightly higher score, but still not that great.\n",
    "\n",
    "\n",
    "the `int_not` `lambda` is a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddf24872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(int_not)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a63a3b8",
   "metadata": {},
   "source": [
    "it is equivalent as the following, but a more compact notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cad826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_not_f(a):\n",
    "    return int(not(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12efb31a",
   "metadata": {},
   "source": [
    "It flips a 0 to a 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7085a88e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_not(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7def794",
   "metadata": {},
   "source": [
    "and th eother way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea0e36e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_not(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3c53d51",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_df\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_text_Medium\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecile_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_df' is not defined"
     ]
    }
   ],
   "source": [
    "compas_df.groupby('score_text_Medium')['decile_score'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab356fe4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m compas_race \u001b[38;5;241m=\u001b[39m \u001b[43mcompas_df\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_df' is not defined"
     ]
    }
   ],
   "source": [
    "compas_race = compas_df.groupby('race')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acadf72",
   "metadata": {},
   "source": [
    "## Per Group scores with groupby\n",
    "\n",
    "To groupby and then do the score, we can use a lambda again, with apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8e9cdcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_race' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m acc_fx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m d: skmetrics\u001b[38;5;241m.\u001b[39maccuracy_score(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo_year_recid\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      2\u001b[0m              d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_text_MedHigh\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 4\u001b[0m \u001b[43mcompas_race\u001b[49m\u001b[38;5;241m.\u001b[39mapply(acc_fx,)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_race' is not defined"
     ]
    }
   ],
   "source": [
    "acc_fx = lambda d: skmetrics.accuracy_score(d['two_year_recid'],\n",
    "             d['score_text_MedHigh'])\n",
    "\n",
    "compas_race.apply(acc_fx,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7ac04f",
   "metadata": {},
   "source": [
    "In this case it gives a series, but with `reset_index` we can make it a DataFrame and then rename the column to label it as accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8672ee2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_race' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_race\u001b[49m\u001b[38;5;241m.\u001b[39mapply(acc_fx,)\u001b[38;5;241m.\u001b[39mreset_index()\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m0\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_race' is not defined"
     ]
    }
   ],
   "source": [
    "compas_race.apply(acc_fx,).reset_index().rename(columns={0:'accuracy'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba90199",
   "metadata": {},
   "source": [
    "That lambda + apply is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dff6f516",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_race' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m race_acc \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m race, rdf \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcompas_race\u001b[49m:\n\u001b[1;32m      3\u001b[0m     acc \u001b[38;5;241m=\u001b[39m skmetrics\u001b[38;5;241m.\u001b[39maccuracy_score(rdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo_year_recid\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      4\u001b[0m              rdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_text_MedHigh\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m     race_acc\u001b[38;5;241m.\u001b[39mappend([race,acc])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_race' is not defined"
     ]
    }
   ],
   "source": [
    "race_acc = []\n",
    "for race, rdf in compas_race:\n",
    "    acc = skmetrics.accuracy_score(rdf['two_year_recid'],\n",
    "             rdf['score_text_MedHigh'])\n",
    "    race_acc.append([race,acc])\n",
    "\n",
    "pd.DataFrame(race_acc, columns =['race','accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0795bc50",
   "metadata": {},
   "source": [
    "## Using AIF360\n",
    "\n",
    "The AIF360 package implements fairness metrics, some of which are derived from metrics we have seen and some others. [the documentation](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.ClassificationMetric.html#aif360.metrics.ClassificationMetric) has the full list in a summary table with English explanations and details with most equations.\n",
    "\n",
    "However, it has a few requirements:\n",
    "- its constructor takes two `BinaryLabelDataset` objects\n",
    "- these objects must be the same except for the label column\n",
    "- the constructor for `BinaryLabelDataset` only accepts all numerical DataFrames\n",
    "\n",
    "\n",
    "So, we have some preparation to do.  \n",
    "\n",
    "\n",
    "First, we'll make a numerical copy of the `compas_df` columns that we need. The only nonnumerical column that we need is race, wo we'll make a `dict` to replace that/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90b37518",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m race_num_map \u001b[38;5;241m=\u001b[39m {r:i \u001b[38;5;28;01mfor\u001b[39;00m i,r, \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mcompas_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mindex)}\n\u001b[1;32m      2\u001b[0m race_num_map\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_df' is not defined"
     ]
    }
   ],
   "source": [
    "race_num_map = {r:i for i,r, in enumerate(compas_df['race'].value_counts().index)}\n",
    "race_num_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae76eaa",
   "metadata": {},
   "source": [
    "and here we select columns and replace the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a3a8f2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m required_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo_year_recid\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_text_MedHigh\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m num_compas \u001b[38;5;241m=\u001b[39m \u001b[43mcompas_df\u001b[49m[required_cols]\u001b[38;5;241m.\u001b[39mreplace(race_num_map)\n\u001b[1;32m      3\u001b[0m num_compas\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_df' is not defined"
     ]
    }
   ],
   "source": [
    "required_cols = ['race','two_year_recid','score_text_MedHigh']\n",
    "num_compas = compas_df[required_cols].replace(race_num_map)\n",
    "num_compas.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428ece5e",
   "metadata": {},
   "source": [
    "Next we will make two versions, one with race & the ground truth and ht eother with race & the predictions. It's easiest to drop the column we don't want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "271df218",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_compas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m num_compas_true \u001b[38;5;241m=\u001b[39m \u001b[43mnum_compas\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_text_MedHigh\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m num_compas_pred \u001b[38;5;241m=\u001b[39m num_compas\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo_year_recid\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_compas' is not defined"
     ]
    }
   ],
   "source": [
    "num_compas_true = num_compas.drop(columns=['score_text_MedHigh'])\n",
    "num_compas_pred = num_compas.drop(columns=['two_year_recid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8486ef0",
   "metadata": {},
   "source": [
    "Now we make the [`BinaryLabelDataset`](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.datasets.BinaryLabelDataset.html#aif360.datasets.BinaryLabelDataset) objects, this type comes from AIF360 too.  Basically, it is a DataFrame with extra attributes; some specific and some inherited from [`StructuredDataset`](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.datasets.StructuredDataset.html#aif360.datasets.StructuredDataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2345c280",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BinaryLabelDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# here we want actual favorable outcome\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m broward_true \u001b[38;5;241m=\u001b[39m \u001b[43mBinaryLabelDataset\u001b[49m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,df \u001b[38;5;241m=\u001b[39m num_compas_true,\n\u001b[1;32m      3\u001b[0m           label_names\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo_year_recid\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      4\u001b[0m          protected_attribute_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m compas_predictions \u001b[38;5;241m=\u001b[39m BinaryLabelDataset(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,df \u001b[38;5;241m=\u001b[39m num_compas_pred,\n\u001b[1;32m      6\u001b[0m           label_names\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_text_MedHigh\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      7\u001b[0m          protected_attribute_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BinaryLabelDataset' is not defined"
     ]
    }
   ],
   "source": [
    "# here we want actual favorable outcome\n",
    "broward_true = BinaryLabelDataset(0,1,df = num_compas_true,\n",
    "          label_names= ['two_year_recid'],\n",
    "         protected_attribute_names=['race'])\n",
    "compas_predictions = BinaryLabelDataset(0,1,df = num_compas_pred,\n",
    "          label_names= ['score_text_MedHigh'],\n",
    "         protected_attribute_names=['race'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d41785d",
   "metadata": {},
   "source": [
    "```{admonition} Try it Yourself\n",
    "remember, you can inspect *any* object using the `__dict__` attribute\n",
    "```\n",
    "\n",
    "\n",
    "This type also has an `ignore_fields` column for when comparisons are made, since the requirement is that only the *content* of the label column is different, but in our case also the label names are different, we have to tell it that that's okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfc4ff59",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_predictions\u001b[49m\u001b[38;5;241m.\u001b[39mignore_fields\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_names\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m broward_true\u001b[38;5;241m.\u001b[39mignore_fields\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_names\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "compas_predictions.ignore_fields.add('label_names')\n",
    "broward_true.ignore_fields.add('label_names')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c5dfa5",
   "metadata": {},
   "source": [
    "Now, we can instantiate our metric object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36b2b8b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fairmetrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m compas_fair_scorer \u001b[38;5;241m=\u001b[39m \u001b[43mfairmetrics\u001b[49m\u001b[38;5;241m.\u001b[39mClassificationMetric(broward_true,\n\u001b[1;32m      2\u001b[0m                            compas_predictions,\n\u001b[1;32m      3\u001b[0m                  unprivileged_groups\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0\u001b[39m}],\n\u001b[1;32m      4\u001b[0m                 privileged_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m1\u001b[39m}])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fairmetrics' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer = fairmetrics.ClassificationMetric(broward_true,\n",
    "                           compas_predictions,\n",
    "                 unprivileged_groups=[{'race':0}],\n",
    "                privileged_groups = [{'race':1}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a364d56e",
   "metadata": {},
   "source": [
    "And finally we can compute! First, we can verify that we get the same accuracy as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "faed53dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39maccuracy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc219c3e",
   "metadata": {},
   "source": [
    "For the aif360 metrics, they have one parameter, `privleged` with a defautl value of `None` when it's none it computes th ewhole dataset.  When `True` it compues only the priveleged group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b073f455",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39maccuracy(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.accuracy(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0751515",
   "metadata": {},
   "source": [
    "Here that is Caucasion people.\n",
    "\n",
    "\n",
    "When `False` it's the unpriveleged group, here African American"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "867bad56",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39maccuracy(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.accuracy(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a34537",
   "metadata": {},
   "source": [
    "We can also compute other scores.  Many fairness scores are ratios of the un priveleged group's score to the privleged group's score.  \n",
    "\n",
    "In Disparate Impact the ratio is of the positive outcome, independent of the predictor.  So this is the ratio of the % of Black people not rearrested to % of white people rearrested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27880428",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39mdisparate_impact()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.disparate_impact()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a886032",
   "metadata": {},
   "source": [
    "The courts use an \"80%\" rule saying that if this ratio is above .8 for things like employment, it's close enough. T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59b4eaa7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39merror_rate_ratio()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.error_rate_ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c532f46",
   "metadata": {},
   "source": [
    "We can also do ratios of the scores.  This is where the journalists [found bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c30aa2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39mfalse_positive_rate_ratio()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.false_positive_rate_ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a6ed3c",
   "metadata": {},
   "source": [
    "Black people were given a low score and then re-arrested only a little more than half as often as white people.  (White people were give an low score and rearrested almost twice as often)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f3d0d0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39mfalse_negative_rate_ratio()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.false_negative_rate_ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee798760",
   "metadata": {},
   "source": [
    "Black people were given a high score and not rearrested almost twice as often as white people.\n",
    "\n",
    "So while the accuracy was similar (see error rate ratio) for Black and White people; the algorithm makes the opposite types of errors.  \n",
    "\n",
    "After the journalists published the piece, the people who made COMPAS countered with a technical report, arguing that that the journalists had measured fairness incorrectly.\n",
    "\n",
    "The journalists two measures false positive rate and false negative rate use the true outcomes as the denominator.  \n",
    "\n",
    "The [COMPAS creators argued](https://www.equivant.com/response-to-propublica-demonstrating-accuracy-equity-and-predictive-parity/) that the model should be evaluated in terms of if a given score means the same thing across races; using the prediction as the denominator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44d8b23c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39mfalse_omission_rate_ratio()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.false_omission_rate_ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66c8ad5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compas_fair_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompas_fair_scorer\u001b[49m\u001b[38;5;241m.\u001b[39mfalse_discovery_rate_ratio()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compas_fair_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "compas_fair_scorer.false_discovery_rate_ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ddf039",
   "metadata": {},
   "source": [
    "On these two metrics, the ratio is closer to 1 and much less disparate.\n",
    "\n",
    "\n",
    "The creators thought it was important for the score to mean the same thing for every person assigned a score. The journalists thought it was more important for the algorithm to have the same impact of different groups of people.  \n",
    "Ideally, we would like the score to both mean the same thing for different people and to have the same impact.  \n",
    "\n",
    "Researchers established that these are mutually exclusive, provably.  We cannot have both, so it is very important to think about what the performance metrics mean and how your algorithm will be used in order to choose how to prepare a model.  We will train models starting next week, but knowing these goals in advance is essential.\n",
    "\n",
    "Importantly, this is not a statistical, computational choice that data can answer for us. This is about *human* values (and to some extent the law; certain domains have legal protections that require a specific condition).\n",
    "\n",
    "The Fair Machine Learning book's classificaiton Chapter has a [section on relationships between criteria](https://fairmlbook.org/classification.html#relationships-between-criteria) with the proofs.\n",
    "\n",
    "\n",
    "```{important}\n",
    "\n",
    "We used ProPublica's COMPAS dataset to replicate (parts of, with different tools) their analysis. That is, they collected the dataset in order to audit the COMPAS algorithm and we used it for the same purpose (and to learn model evaluation).  This dataset is not designed for *training* models, even though it has been used as such many times.  This is [not the best way](https://openreview.net/pdf?id=qeM58whnpXM) to use this dataset and for future assignments I do not recommend using this dataset.\n",
    "```\n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "If you are interested in fairness in ML, that is what my research is. Aiden has been working on a project with me since summer and I'll be taking new students in the spring.  Students who have completed this course are excellent candidates to join my lab.  Let me know if you are interested!\n",
    "```\n",
    "````\n",
    "\n",
    "\n",
    "## Portfolio Reminder\n",
    "\n",
    "If you do not need level 3s to be happy with your grade for the course (eg you want a B) and you have all the achievements so far you can skip the portfolio submission.  If you do not need level 3 and you are not on track, you should submit to get caught up. This can be (and is advised to be) reflective revisions of past assignment(s). \n",
    "\n",
    "If you need level 3 achievements for your desired grade, then you can pick a [subset of the eligible skills](https://rhodyprog4ds.github.io/BrownFall22/portfolio/index.html#upcoming-checks)(or all)  and add *new*  work that shows that you have learned those skills according to the [level 3 checklists](https://rhodyprog4ds.github.io/BrownFall22/syllabus/achievements.html#detailed-checklists).  [the ideas page](https://rhodyprog4ds.github.io/BrownFall22/portfolio/check1ideas.html) has example formats for that new work.\n",
    "\n",
    "\n",
    "## Questions After Class\n",
    "\n",
    "Today's questions were only clarifying, so hopefully re-reading the notes is enough.  If not, post a question as an issue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6366dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.14.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "source_map": [
   12,
   32,
   44,
   51,
   53,
   56,
   58,
   66,
   71,
   78,
   80,
   84,
   87,
   91,
   93,
   95,
   99,
   103,
   105,
   111,
   116,
   119,
   121,
   125,
   133,
   151,
   154,
   157,
   162,
   165,
   168,
   171,
   179,
   187,
   190,
   193,
   198,
   201,
   203,
   207,
   209,
   215,
   217,
   223,
   225,
   229,
   231,
   236,
   238,
   242,
   244,
   257,
   261,
   264,
   303
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}