---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.14.1
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Decision Trees

```{code-cell} ipython3
%matplotlib inline
```

```{code-cell} ipython3
import pandas as pd
import seaborn as sns
import numpy as np
from sklearn import tree
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn import metrics
sns.set(palette='colorblind') # this improves contrast

from sklearn.metrics import confusion_matrix, classification_report
```

## Let's look at a toy dataset

Using a toy dataset here shows an easy to see challenge for the classifier that we have seen so far.  Real datasets will be hard in different ways, and since they're higher dimensional, it's harder to visualize the cause.

```{code-cell} ipython3
corner_data = 'https://raw.githubusercontent.com/rhodyprog4ds/06-naive-bayes/f425ba121cc0c4dd8bcaa7ebb2ff0b40b0b03bff/data/dataset6.csv'
df6= pd.read_csv(corner_data,usecols=[1,2,3])
```

```{code-cell} ipython3
gnb = GaussianNB()
```

```{code-cell} ipython3
sns.pairplot(data=df6, hue='char',hue_order=['A','B'])
```

As we can see in this dataset, these classes are quite separated.

```{code-cell} ipython3
X_train, X_test, y_train, y_test = train_test_split(df6[['x0','x1']],
                          df6['char'],
                          random_state = 4)
```

```{code-cell} ipython3
gnb.fit(X_train,y_train)
gnb.score(X_test,y_test)
```

But we do not get a very good classification score.

To see why, we can look at what it learned.

```{code-cell} ipython3
gnb.__dict__
```

```{code-cell} ipython3
N = 100
gnb_df = pd.DataFrame(np.concatenate([np.random.multivariate_normal(th, sig*np.eye(2),N)
         for th, sig in zip(gnb.theta_,gnb.sigma_)]),
         columns = ['x0','x1'])
gnb_df['char'] = [ci for cl in [[c]*N for c in gnb.classes_] for ci in cl]

sns.pairplot(data =gnb_df, hue='char',hue_order=['A','B'])
```

This does not look much like the data and it's hard to tell which is higher at any given point in the 2D space.  We know though, that it has missed the mark. We can also look at the actual predictions.

```{code-cell} ipython3
df6pred = X_test.copy()
df6pred['pred'] =gnb.predict(X_test)

sns.pairplot(df6pred, hue = 'pred',hue_order=['A','B'])
```

This makes it more clear. It basically learns one group that covers 3 blobs and only 1 for the second color. If we train again with a different random seed, it makes a different specific error, but basically the same idea. 

```{code-cell} ipython3
X_train, X_test, y_train, y_test = train_test_split(df6[['x0','x1']],
                          df6['char'], random_state = 5)
gnb.fit(X_train,y_train)
gnb.score(X_test,y_test)
```

```{code-cell} ipython3
df6pred = X_test.copy()
df6pred['pred'] =gnb.predict(X_test)

sns.pairplot(df6pred, hue = 'pred')
```

```{code-cell} ipython3
X_train, X_test, y_train, y_test = train_test_split(df6[['x0','x1']],
                          df6['char'], random_state = 7)
gnb.fit(X_train,y_train)
gnb.score(X_test,y_test)
```

```{code-cell} ipython3
df6pred = X_test.copy()
df6pred['pred'] =gnb.predict(X_test)

sns.pairplot(df6pred, hue = 'pred')
```

If you try this again, split, fit, plot, it will learn different decisions, but always at least about 25% of the data will have to be classified incorrectly.

## Decision Trees

This data does not fit the assumptions of the Niave Bayes model, but a decision tree has a different rule. It can be more complex, but for the scikit learn one relies on splitting the data at a series of points along one axis at a time.  

It is a **discriminative** model, because it describes how to discriminate (in the sense of differentiate) between the classes.

```{code-cell} ipython3
dt = tree.DecisionTreeClassifier()
```

```{code-cell} ipython3
dt.fit(X_train,y_train)
```

```{code-cell} ipython3
dt.score(X_test,y_test)
```

The sklearn estimator objects (that corresond to different models) all have the same API, so the `fit`, `predict`, and `score` methods are the same as above. We will see this also in regression and clustering.  What each method does in terms of the specific calculations will vary depending on the model, but they're always there.  

the `tree` module also allows you to plot the tree to examine it.

```{code-cell} ipython3
plt.figure(figsize=(15,20))
tree.plot_tree(dt, rounded =True, class_names = ['A','B'],
      proportion=True, filled =True, impurity=False,fontsize=10);
```

````{margin}
```{note}
On the iris dataset, the [sklearn docs include a diagram showing the decision boundary](https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html) You should be able to modify this for another classifier.

```
````

## Setting Classifier Parameters

The decision tree we had above has a lot more layers than we would expect.  This is really simple data so we still got perfect classification. However, the more complex the model, the more risk that it will learn something noisy about the training data that doesn't hold up in the test set.  

Fortunately, we can control the parameters to make it find a simpler decision boundary.

```{code-cell} ipython3
dt2 = tree.DecisionTreeClassifier(max_depth=2)
dt2.fit(X_train,y_train)
```

```{code-cell} ipython3
plt.figure(figsize=(15,20))
tree.plot_tree(dt2, rounded =True, class_names = ['A','B'],
      proportion=True, filled =True, impurity=False,fontsize=10);
```

```{code-cell} ipython3
dt2.score(X_test,y_test)
```

We might need to play with different parameters to get it just how we want it.  A simpler model is better because it will be more reliable in general.

```{code-cell} ipython3
dt2.score(X_test,y_test)
```

```{code-cell} ipython3
dt2_20 = tree.DecisionTreeClassifier(max_depth=2)
dt2_20.fit(X_train,y_train)
```

```{code-cell} ipython3
dt2_20.score(X_test, y_test)
```

```{code-cell} ipython3
plt.figure(figsize=(15,20))
tree.plot_tree(dt2_20, rounded =True, class_names = ['A','B'],
      proportion=True, filled =True, impurity=False,fontsize=10);
```

## Questions After Class


### What do the dots and lines represent in the graph?  

The scatter plots, each dot is one sample (row from the DataFrame), the different sub plots are different views of the data, determined by different pairs of variables.

The lines are the density, when it's high it means values are likely , when it's low it means they are unlikely.

### What doe the GNB model do?

When we fit, it learns a description of the data.  When we predict it uses that description to predict the probability that each test sample belongs to each of the classes and returns the most probable one.  See the last notes for a visualization of the probabilities.

### Is xtrain ytrain, xtest ytest the same every time?

Generally no, with the `random_state` variable set on a given data they will be the same subset every time, but without it would get different rows.  For example:

```{code-cell} ipython3
X_train, X_test, y_train, y_test = train_test_split(df6[['x0','x1']],
                          df6['char'],
                          random_state = 4)
X_train.head()
```

```{code-cell} ipython3
X_train, X_test, y_train, y_test = train_test_split(df6[['x0','x1']],
                          df6['char'],
                          random_state = 4)
X_train.head()
```

with random state, we get the same samples each time.

```{code-cell} ipython3
X_train, X_test, y_train, y_test = train_test_split(df6[['x0','x1']],
                          df6['char'])
X_train.head()
```

```{code-cell} ipython3
X_train, X_test, y_train, y_test = train_test_split(df6[['x0','x1']],
                          df6['char'])
X_train.head()
```

 without, we get different ones.

### I still want to know why it is worse to have a model that gets 100% accuracy with more depth to the decision tree flowchart than to have a model with less accuracy and less decisions in the decision tree.

This accuracy is only on one set of training data, we might not want the big accuracy drop we saw here ,but we could do other things to improve it.
